# ğŸŒ Google-like Translate Transformer â€” *Proof of Concept*

> A Transformer-based neural machine translation system for Italian â†’ English.  
> Trained on Europarl v7 (Parliament Proceedings) with PyTorch, SentencePiece, and a custom-built transformer architecture.  
> Fine-tuned overnight on **NVIDIA H100 SXM** for **$20** on [Vast.ai](https://vast.ai/).

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ saved_model/            # Stores the best checkpoint: best_ckpt.tar
â”œâ”€â”€ sp/                     # SentencePiece models and vocabs
â”‚   â”œâ”€â”€ src_sp.model
â”‚   â”œâ”€â”€ src_sp.vocab
â”‚   â”œâ”€â”€ trg_sp.model
â”‚   â””â”€â”€ trg_sp.vocab
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ run.py              # Main entrypoint for inference
â”‚   â”œâ”€â”€ constants.py        # Paths, vocab size, special tokens
â”‚   â”œâ”€â”€ custom_data.py      # Tokenizer + DataLoader logic
â”‚   â”œâ”€â”€ data_structure.py   # BeamNode definition
â”‚   â”œâ”€â”€ transformer.py      # Full Transformer model
â”‚   â”œâ”€â”€ layers.py           # Attention, FFN layers
â”‚   â””â”€â”€ sentencepiece_train.py  # (Optional) for generating SP models
```

---

## âš™ï¸ Features

- âœ… Transformer (Encoder-Decoder) with Positional Encoding
- âœ… SentencePiece tokenization
- âœ… Beam Search & Greedy Decoding
- âœ… Multi-GPU support (`nn.DataParallel`)
- âœ… Training/Inference logging with timestamps
- âœ… Modular, readable codebase
- âœ… Works well on formal Italian input (legal/gov data)

---

## ğŸ“š Dataset & Training

- ğŸ”— Dataset: [European Parliament Proceedings Parallel Corpus](https://www.statmt.org/europarl/)
- ğŸ—ƒ Size: ~2.1 million aligned sentence pairs
- ğŸ–¥ï¸ Hardware: **NVIDIA H100 SXM (Vast.ai)**
- â±ï¸ Duration: 10 epochs (overnight)
- ğŸ’° Budget: ~\$20

> Because the dataset uses formal jargon, the model performs best when translating well-structured, formal Italian.

---

## ğŸ§ª Inference Usage

### ğŸ”§ Setup

Install dependencies:

```bash
pip install -r requirements.txt
```

### â–¶ï¸ Run Translation

From the project root:

```bash
python src/run.py --model 'best_ckpt.tar' --input 'come stai?' --decode 'beam'
```

Or use greedy decoding:

```bash
python src/run.py --model 'best_ckpt.tar' --input 'come stai?' --decode 'greedy'
```

## ğŸ”„ Example: Beam vs Greedy (Real Outputs)

| Input                          | Greedy Output (Truncated)                                    | Beam Output (Truncated)                             |
|-------------------------------|---------------------------------------------------------------|------------------------------------------------------|
| `come stai?`                  | How are you?'??' How are you???????????????...               | How are you?'??' How are you?                        |
| `sto cercando lavoro`         | I am looking for work job job job job searching job...        | I am looking for work job job job job searching...   |
| `mi chiamo Umberto`           | I am called Umberttobertoberto ometta ometta ometta...        | I am called Umberto Umbertoto Umbertoto be...        |
| `come sta Signor Rossi?`      | How is Mr Rossi?? what is Mr Rosss????? what is Mr...         | How is Mr Rossi?? what is Mr Rossite?                |


> ğŸš¨ Note: Output may include repetitions or struggle with proper punctuation â€” common in smaller Transformer models. That said, **semantic understanding is often surprisingly accurate.**

---

## ğŸ› ï¸ How It Works

### During Training

- Loads SentencePiece tokenizers (`sp/src_sp.model`, `sp/trg_sp.model`)
- Loads vocab files (`.vocab`)
- Initializes a Transformer model with:
  - Self-attention encoder
  - Cross-attention decoder
  - Custom padding/masking logic
- Optimizes with `NLLLoss` and `Adam`
- Saves best checkpoint to `saved_model/best_ckpt.tar`

### During Inference

- Tokenizes input with SentencePiece
- Passes it through encoder
- Decodes using greedy or beam search
- Detokenizes the final sequence

---

## ğŸš€ Future Improvements

- Train on full bigger dataset (i.e. [Tatoeba-Challenge](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/data))
- Fine-tune with general-domain text (e.g. Wikipedia)
- Add punctuation/stop-token rewards (to prevent hallucinations)
- Compress model for faster inference
- Build web UI with `Streamlit`

---

## ğŸ¤ **Looking for contributors**  
> If you're into building affordable AI, experimenting with language models, or just enjoy hacking with transformers â€” you're welcome to help extend this. More languages, more robustness, more research â€” letâ€™s do it together. Contact me on [LinkedIn](https://www.linkedin.com/in/upuddu/).

---

## ğŸ“„ License & Credits

- ğŸ—‚ **Corpus**: [Europarl v7 (StatMT.org)](https://www.statmt.org/europarl/)
- ğŸ’» Framework: PyTorch
- ğŸ“š Tokenization: SentencePiece
- ğŸ§  Author: [Umberto Puddu](https://github.com/umbertopuddu)
- âš–ï¸ License: MIT (default)